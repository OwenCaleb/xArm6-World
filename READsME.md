# xArm6-World
xArm6 simulation environment for RL with a gym-style API.


## Installation

The package can be installed using pip:
```
pip install git+https://github.com/nicklashansen/simxarm.git@main#egg=simxarm
```

Alternatively, you can clone the repository and install an editable version locally:
```
git clone git@github.com:nicklashansen/simxarm.git
cd simxarm
pip install -e .
```

Most dependencies should be installed automatically. However, you may need to install [MuJoCo 2.1.0](https://github.com/deepmind/mujoco/releases/tag/2.1.0) if you do not already have it installed. To install MuJoCo, download it on the link above and make sure it is located at `~/.mujoco/mujoco210`. Then, add the following lines to your `~/.bashrc` file:
```
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:~/.mujoco/mujoco210/bin
```
and similarly place your MuJoCo license key at `~/.mujoco/mjkey.txt`. A free and unrestricted license key can be obtained [here](https://www.roboti.us/file/mjkey.txt).


## Usage

The package provides a gym-style API for controlling the xArm simulation environment. The environment can be initialized as follows:
```
import simxarm
env = simxarm.make(task)
```
where `task` is a string specifying the task to be performed. The following tasks are currently supported:
```



Create a new environment.
Args:
    task (str): The task to create an environment for. Must be one of:
        - 'reach'
        - 'push'
        - 'peg-in-box'
        - 'lift'
    obs_mode (str): The observation mode to use. Must be one of:
        - 'state': Only state observations
        - 'rgb': RGB images
        - 'all': RGB images and state observations
    image_size (int): The size of the image observations
    action_repeat (int): The number of times to repeat the action
    seed (int): The random seed to use
Returns:
    gym.Env: The environment





Tasks:
OrderedDict([('reach', {'env': <class 'simxarm.task.reach.Reach'>, 'action_space': 'xyz', 'episode_length': 50, 'description': 'Reach a target location with the end effector'}), ('push', {'env': <class 'simxarm.task.push.Push'>, 'action_space': 'xyz', 'episode_length': 50, 'description': 'Push a cube to a target location'}), ('peg-in-box', {'env': <class 'simxarm.task.peg_in_box.PegInBox'>, 'action_space': 'xyz', 'episode_length': 50, 'description': 'Insert a peg into a box'}), ('lift', {'env': <class 'simxarm.task.lift.Lift'>, 'action_space': 'xyzw', 'episode_length': 50, 'description': 'Lift a cube above a height threshold'})])

Initialized environment: reach
Observation space: (14,)
Action space: (3,)

Initialized environment: push
Observation space: (35,)
Action space: (3,)

Initialized environment: peg-in-box
Observation space: (35,)
Action space: (3,)

Initialized environment: lift
Observation space: (28,)
Action space: (4,)

Input types:
Observation space state: (28,)
Observation space rgb: (3, 84, 84)
Observation space all: [(3, 84, 84), (4,)]

```
The output above is generated by running `python test.py`. You can verify your installation by running this script.

The `simxarm.make` function is your gateway to the environment. It takes the following arguments:
```
"""
Create a new environment.
Args:
    task (str): The task to create an environment for. Must be one of:
        - 'reach'
        - 'push'
        - 'peg-in-box'
        - 'lift'
    obs_mode (str): The observation mode to use. Must be one of:
        - 'state': Ground-truth state observations
        - 'rgb': RGB images
        - 'all': RGB images and robot state observations
    image_size (int): The size of the image observations
    seed (int): The random seed to use
Returns:
    gym.Env: The environment
""" 
```
The above docstring is taken directly from the code.

To get started with the `simxarm` package, try play around with the `test.py` script and related files. Check that you can run the script without any errors. Then, try changing the task, observation mode, and image size. You can also try to execute random actions in the environment and visualize (render) the observations.


## Acknowledgements
This repository is based on work by [Nicklas Hansen](https://nicklashansen.github.io/), [Yanjie Ze](https://yanjieze.com/), [Rishabh Jangir](https://jangirrishabh.github.io/), [Mohit Jain](https://natsu6767.github.io/), and [Sambaran Ghosal](https://github.com/SambaranRepo) as part of the following publications:
* [Self-Supervised Policy Adaptation During Deployment](https://arxiv.org/abs/2007.04309)
* [Generalization in Reinforcement Learning by Soft Data Augmentation](https://arxiv.org/abs/2011.13389)
* [Stabilizing Deep Q-Learning with ConvNets and Vision Transformers under Data Augmentation](https://arxiv.org/abs/2107.00644)
* [Look Closer: Bridging Egocentric and Third-Person Views with Transformers for Robotic Manipulation](https://arxiv.org/abs/2201.07779)
* [Visual Reinforcement Learning with Self-Supervised 3D Representations](https://arxiv.org/abs/2210.07241)






Mode 1: 1000 (only arm camera)
Mode 2: 0100 (only remote camera)
Mode 3: 0010 (only upview camera)
Mode 4: 0001 (only front camera)
Mode 5: 1100 (arm + remote cameras)
Mode 6: 1010 (arm + upview cameras)
Mode 7: 1001 (arm + front cameras)
Mode 8: 0110 (remote + upview cameras)
Mode 9: 0101 (remote + front cameras)
Mode 10: 0011 (upview + front cameras)
Mode 11: 1110 (arm + remote + upview cameras)
Mode 12: 1101 (arm + remote + front cameras)
Mode 13: 1011 (arm + upview + front cameras)
Mode 14: 0111 (remote + upview + front cameras)
Mode 15: 1111 (all cameras)

		self.mode2camera = {
            "mode1": ['camera1'],      # arm
            "mode2": ['camera0'],      # remote
            "mode3": ['camera2'],      # upview
            "mode4": ['camera3'],      # front
            "mode5": ['camera1', 'camera0'],   # arm + remote
            "mode6": ['camera1', 'camera2'],   # arm + upview
            "mode7": ['camera1', 'camera3'],   # arm + front
            "mode8": ['camera0', 'camera2'],   # remote + upview
            "mode9": ['camera0', 'camera3'],   # remote + front
            "mode10": ['camera2', 'camera3'],  # upview + front
            "mode11": ['camera1', 'camera0', 'camera2'],  # arm + remote + upview
            "mode12": ['camera1', 'camera0', 'camera3'],  # arm + remote + front
            "mode13": ['camera1', 'camera2', 'camera3'],  # arm + upview + front
            "mode14": ['camera0', 'camera2', 'camera3'],  # remote + upview + front
            "mode15": ['camera1', 'camera0', 'camera2', 'camera3'],  # all cameras
        }


因为四元数的增量不能简单地按比例缩放，而是需要确保在每个子步骤内仍然保持四元数的单位长度（即模长为1）。如果不遵循这一要求，四元数的插值可能会导致不符合规范的结果，进而影响模拟的稳定性和准确性。
为了在多个子步骤内逐渐增加四元数增量，同时确保四元数的单位长度，我们可以使用 球面线性插值（Slerp） 来进行逐渐增量的应用。这种方法可以保证四元数增量的模长保持为1，并且在多个子步骤内平滑地过渡。

1. 四元数单位化（Normalization）
首先，四元数的模长必须为 1。如果你的增量四元数在每次加法后不再是单位四元数，你需要对其进行单位化操作。对于每个子步骤的增量，可以对其进行归一化，以确保它保持单位长度。

2. 球面线性插值（Slerp）
Slerp（球面线性插值）是一种确保四元数之间平滑插值的技术。通过它，我们可以平滑地将当前四元数旋转到目标四元数。

3. 方法
你可以按以下步骤逐渐更新四元数：

将当前四元数和目标四元数（增量后的四元数）之间的增量通过 Slerp 进行逐步插值。
每次在一个子步骤内应用一定比例的增量，确保四元数的模长保持为 1。


		self._mujoco_depth_min=0.009999999776482582
		self._mujoco_depth_max=50.0
		self._extent=5.000024999687508


        action_control_mode simple 和complex


要安装你提供的包，可以进入包含 setup.py 文件的目录，并执行以下命令：

bash
复制代码
pip install .
或者，如果你希望以开发模式安装（即允许你在安装后进行代码修改，而无需重新安装），可以运行：

bash
复制代码
pip install -e .